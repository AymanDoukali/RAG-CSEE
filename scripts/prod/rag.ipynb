{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3889039a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4cb4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the script environment\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append('./utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd48dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "from typing import List, Dict, Optional, Any\n",
    "from logging_config import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b812288",
   "metadata": {},
   "source": [
    "## 1. Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574311ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from utils.text_extractor import TextExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e4c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../src/first_batch/IG03056_V2.pdf\"\n",
    "\n",
    "text_extractor = TextExtractor(file_path, min_words=20)\n",
    "\n",
    "text_extractor.extract_text_advanced()\n",
    "text_extractor.clean_text()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fec07e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text_extractor.get_text()\n",
    "print(text[0:1000])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad82ec7",
   "metadata": {},
   "source": [
    "## 2. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebdf73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from embeddings import get_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c21378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_semantic(pages: List[Dict], embeddings, breakpoint_threshold_type = \"percentile\", breakpoint_threshold_amount: float = 0.95, min_chunk_size: int = 100) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Splits page texts using LangChain's SemanticChunker.\n",
    "    Groups sentences based on semantic similarity, not just size.\n",
    "\n",
    "    Parameters:\n",
    "        pages: List of dicts with keys: doc_name, page_num, text\n",
    "        embeddings: The embeddings model to use for semantic chunking\n",
    "        breakpoint_threshold_type: Type of threshold for chunking (default is \"percentile\")\n",
    "        breakpoint_threshold_amount: Amount for the threshold (default is 0.95)\n",
    "        min_chunk_size: Minimum size of chunks (default is 100)\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with chunked text and metadata\n",
    "    \"\"\"\n",
    "    chunker = SemanticChunker(embeddings, breakpoint_threshold_type=breakpoint_threshold_type, breakpoint_threshold_amount=breakpoint_threshold_amount, min_chunk_size=min_chunk_size)\n",
    "\n",
    "    chunks = []\n",
    "    for page in pages:\n",
    "        try:\n",
    "            text = page.get(\"text\", \"\")\n",
    "            chunked_texts = chunker.split_text(text)\n",
    "            for i, chunk in enumerate(chunked_texts):\n",
    "                chunks.append({\n",
    "                    \"doc_name\": page.get(\"doc_name\", \"unknown\"),\n",
    "                    \"page_number\": page.get(\"page_number\", -1),\n",
    "                    \"chunk_id\": f\"{doc_name}_p{page_num}_c{i}\",\n",
    "                    \"text\": chunk,\n",
    "                    \"num_words\": len(chunk.split()),\n",
    "                    \"extraction_method\": page.get(\"extraction_method\", \"unknown\"),\n",
    "                    \"table_bboxes\": page.get(\"table_bboxes\", [])\n",
    "                })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"⚠️ Semantic chunking failed on {doc_name} page {page_num}: {e}\")\n",
    "\n",
    "    logger.info(f\"✅ Created {len(chunks)} semantic chunks from {len(pages)} pages in {doc_name}.\\n\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbb488",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a7464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_pages = text_extractor.get_data()\n",
    "chunks = chunk_text_semantic(doc_pages, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb154e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f34aa4",
   "metadata": {},
   "source": [
    "## 3. Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd991dd",
   "metadata": {},
   "source": [
    "#### 3.1. Documents preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccd3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f94b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting chunks into LangChain Documents\n",
    "def prepare_documents(chunks: List[Dict]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Converts each chunk dict to a LangChain Document object\n",
    "    with metadata (doc_name, page_num, chunk_id)\n",
    "\n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries\n",
    "\n",
    "    Returns:\n",
    "        List of LangChain Document objects\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for chunk in chunks:\n",
    "        metadata = {\n",
    "            \"doc_name\": chunk.get(\"doc_name\", \"unknown\"),\n",
    "            \"page_number\": chunk.get(\"page_number\", -1),\n",
    "            \"chunk_id\": chunk.get(\"chunk_id\", \"unknown\"),\n",
    "            \"text\": chunk.get(\"text\", \"\"),\n",
    "            \"num_words\": chunk.get(\"num_words\", 0),\n",
    "            \"extraction_method\": chunk.get(\"extraction_method\", \"unknown\"),\n",
    "            \"table_bboxes\": chunk.get(\"table_bboxes\", []),\n",
    "        }\n",
    "        documents.append(Document(page_content=chunk[\"text\"], metadata=metadata))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb037a7",
   "metadata": {},
   "source": [
    "#### 3.2. Build the FAISS index from Documents using the embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e69e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the FAISS index from Documents\n",
    "FAISS_PATH = \"faiss_index\"\n",
    "\n",
    "def build_faiss_index(documents: List[Document], embeddings: HuggingFaceEmbeddings, persist_path: str = FAISS_PATH) -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS vector store from a list of LangChain Documents.\n",
    "    Saves it to disk for future use.\n",
    "\n",
    "    Args:\n",
    "        documents: List of LangChain Document objects\n",
    "        embeddings: The embeddings model to use for FAISS indexing\n",
    "        persist_path: Path to save the FAISS index\n",
    "\n",
    "    Returns:\n",
    "        FAISS vector store\n",
    "    \"\"\"\n",
    "    # Build the index in memory\n",
    "    vectorstore = FAISS.from_documents(documents, embedding=embeddings)\n",
    "\n",
    "    # Save to disk\n",
    "    vectorstore.save_local(persist_path)\n",
    "    print(f\"✅ FAISS index built and saved to '{persist_path}'\\n\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = prepare_documents(chunks)\n",
    "build_faiss_index(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f5459",
   "metadata": {},
   "source": [
    "#### 3.3. Visualize the FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf2833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d28ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.load_local(FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "documents = list(vectorstore.docstore._dict.values())\n",
    "metadatas = [doc.metadata for doc in documents]\n",
    "\n",
    "all_embeddings = vectorstore.index.reconstruct_n(0, vectorstore.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d10c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_2D():\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    reduced_vectors = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "    # Create the 2D scatter plot\n",
    "    fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, opacity=0.8),\n",
    "    text=[\n",
    "        f\"doc_name: {doc.metadata.get('doc_name', '')}\\t\"\n",
    "        f\"page: {doc.metadata.get('page_number', '')}\\t\"\n",
    "        f\"n_words: {doc.metadata.get('num_words', 0)}<br>\"\n",
    "        f\"text: {doc.page_content[:150]}...\"\n",
    "        for doc in documents\n",
    "    ],\n",
    "     hoverinfo='text'\n",
    "    )])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='2D FAISS Vector Store Visualization',\n",
    "        scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "        width=800,\n",
    "        height=600,\n",
    "        margin=dict(r=20, b=10, l=10, t=40)\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebbb2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_2D()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
