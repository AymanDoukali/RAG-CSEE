{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3889039a",
   "metadata": {},
   "source": [
    "# RAG SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4cb4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the script environment\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append('./utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd48dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "from typing import List, Dict, Optional, Any\n",
    "from logging_config import logger\n",
    "from io_docs import save_dict_list_as_json, load_json_as_dict_list, dict_list_to_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b812288",
   "metadata": {},
   "source": [
    "## 1. Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574311ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from utils.text_extractor import TextExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e4c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_FOLDER_PATH = \"../../src/first_batch/\"\n",
    "OUTPUT_FOLDER_PATH = \"./output/extracted_text/\"\n",
    "LIMIT = 0\n",
    "n_extracted = 0\n",
    "\n",
    "for file_name in os.listdir(SRC_FOLDER_PATH):\n",
    "    if file_name.endswith('.pdf') and (n_extracted < LIMIT or LIMIT == -1):\n",
    "        file_path = os.path.join(SRC_FOLDER_PATH, file_name)\n",
    "\n",
    "        if f\"{os.path.basename(file_path).split('.')[0]}_extracted.json\" in os.listdir(OUTPUT_FOLDER_PATH):\n",
    "            logger.info(f\"Skipping already processed file: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"Processing file: {file_path}\")\n",
    "\n",
    "        # Initialize the TextExtractor with the file path and parameters\n",
    "        text_extractor = TextExtractor(file_path, min_words=14)\n",
    "        \n",
    "        # Extract text from the PDF\n",
    "        text_extractor.extract_text_advanced()\n",
    "        \n",
    "        # Clean the extracted text\n",
    "        text_extractor.clean_text()\n",
    "\n",
    "        # Saving extracted text to folder\n",
    "        os.makedirs(OUTPUT_FOLDER_PATH, exist_ok=True)\n",
    "        output_file_path = os.path.join(OUTPUT_FOLDER_PATH, f\"{os.path.basename(file_path).split('.')[0]}_extracted.json\")\n",
    "        save_dict_list_as_json(text_extractor.get_data(), output_file_path)\n",
    "\n",
    "        n_extracted += 1\n",
    "\n",
    "    if n_extracted >= LIMIT and LIMIT != -1:\n",
    "        logger.info(f\"Reached limit of {LIMIT} files processed. Stopping.\")\n",
    "        break\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad82ec7",
   "metadata": {},
   "source": [
    "## 2. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebdf73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from embeddings import get_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c21378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_semantic(pages: List[Dict], embeddings, breakpoint_threshold_type = \"percentile\", breakpoint_threshold_amount: float = 95, min_chunk_size: int = 100) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Splits page texts using LangChain's SemanticChunker.\n",
    "    Groups sentences based on semantic similarity, not just size.\n",
    "    Information on how to use the chunker can be found in the LangChain documentation: https://python.langchain.com/docs/how_to/semantic-chunker/\n",
    "\n",
    "    Parameters:\n",
    "        pages: List of dicts with keys: doc_name, page_num, text\n",
    "        embeddings: The embeddings model to use for semantic chunking\n",
    "        breakpoint_threshold_type: Type of threshold for chunking (default is \"percentile\")\n",
    "        breakpoint_threshold_amount: Amount for the threshold (default is 95)\n",
    "        min_chunk_size: Minimum size of chunks (default is 100)\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with chunked text and metadata\n",
    "    \"\"\"\n",
    "    chunker = SemanticChunker(embeddings, breakpoint_threshold_type=breakpoint_threshold_type, breakpoint_threshold_amount=breakpoint_threshold_amount, min_chunk_size=min_chunk_size) # type: ignore\n",
    "\n",
    "    chunks = []\n",
    "    doc_name = pages[0].get(\"doc_name\", \"unknown\") if pages else \"unknown\"\n",
    "\n",
    "    for page in pages:\n",
    "        page_num = page.get(\"page_number\", -1)\n",
    "        try:\n",
    "            text = page.get(\"text\", \"\")\n",
    "            chunked_texts = chunker.split_text(text)\n",
    "            for i, chunk in enumerate(chunked_texts):\n",
    "                chunks.append({\n",
    "                    \"doc_name\": doc_name,\n",
    "                    \"page_number\": page_num,\n",
    "                    \"chunk_id\": f\"{doc_name}_p{page_num}_c{i}\",\n",
    "                    \"text\": chunk,\n",
    "                    \"num_words\": len(chunk.split()),\n",
    "                    \"extraction_method\": page.get(\"extraction_method\", \"unknown\"),\n",
    "                    \"table_bboxes\": page.get(\"table_bboxes\", [])\n",
    "                })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ö†Ô∏è Semantic chunking failed on {doc_name} page {page_num}: {e}\")\n",
    "\n",
    "    logger.info(f\"‚úÖ Created {len(chunks)} semantic chunks from {len(pages)} pages in {doc_name}.\\n\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbb488",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embedding_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a7464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_FOLDER_PATH = \"./output/extracted_text/\"\n",
    "OUTPUT_FOLDER_PATH = \"./output/chunked_text/\"\n",
    "LIMIT = 0\n",
    "n_chunked = 0\n",
    "\n",
    "os.makedirs(OUTPUT_FOLDER_PATH, exist_ok=True)\n",
    "\n",
    "for i, doc in enumerate(os.listdir(SRC_FOLDER_PATH)):\n",
    "    if doc.endswith('.json') and (n_chunked < LIMIT or LIMIT == -1):\n",
    "        file_path = os.path.join(SRC_FOLDER_PATH, doc)\n",
    "\n",
    "        if f\"{os.path.basename(file_path).split('.')[0]}_chunked.json\" in os.listdir(OUTPUT_FOLDER_PATH):\n",
    "            logger.info(f\"Skipping already processed file: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"Processing file: {file_path}\")\n",
    "\n",
    "        # Load the extracted text data\n",
    "        doc_pages = load_json_as_dict_list(file_path)\n",
    "\n",
    "        # Chunk the text semantically\n",
    "        chunks = chunk_text_semantic(doc_pages, embeddings, min_chunk_size=200, breakpoint_threshold_type=\"gradient\")\n",
    "\n",
    "        # Save the chunks to a new JSON file\n",
    "        output_file_path = os.path.join(OUTPUT_FOLDER_PATH, f\"{os.path.basename(file_path).split('.')[0]}_chunked.json\")\n",
    "        save_dict_list_as_json(chunks, output_file_path)\n",
    "\n",
    "        n_chunked += 1\n",
    "\n",
    "    if n_chunked >= LIMIT and LIMIT != -1:\n",
    "        logger.info(f\"Reached limit of {LIMIT} files processed. Stopping.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f34aa4",
   "metadata": {},
   "source": [
    "## 3. Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd991dd",
   "metadata": {},
   "source": [
    "#### 3.1. Documents preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccd3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f94b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting chunks into LangChain Documents\n",
    "def prepare_documents(chunks: List[Dict]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Converts each chunk dict to a LangChain Document object\n",
    "    with metadata (doc_name, page_num, chunk_id)\n",
    "\n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries\n",
    "\n",
    "    Returns:\n",
    "        List of LangChain Document objects\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for chunk in chunks:\n",
    "        metadata = {\n",
    "            \"doc_name\": chunk.get(\"doc_name\", \"unknown\"),\n",
    "            \"page_number\": chunk.get(\"page_number\", -1),\n",
    "            \"chunk_id\": chunk.get(\"chunk_id\", \"unknown\"),\n",
    "            \"text\": chunk.get(\"text\", \"\"),\n",
    "            \"num_words\": chunk.get(\"num_words\", 0),\n",
    "            \"extraction_method\": chunk.get(\"extraction_method\", \"unknown\"),\n",
    "            \"table_bboxes\": chunk.get(\"table_bboxes\", []),\n",
    "        }\n",
    "        documents.append(Document(page_content=chunk[\"text\"], metadata=metadata))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb037a7",
   "metadata": {},
   "source": [
    "#### 3.2. Build the FAISS index from Documents using the embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "FAISS_PATH = \"faiss_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e69e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the FAISS index from Documents\n",
    "def build_faiss_index(documents: List[Document], embeddings: HuggingFaceEmbeddings, persist_path: str = FAISS_PATH) -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates or updates a FAISS vector store from a list of LangChain Documents.\n",
    "    If the index already exists, only adds new documents instead of recreating it.\n",
    "    Saves it to disk for future use.\n",
    "\n",
    "    Args:\n",
    "        documents: List of LangChain Document objects\n",
    "        embeddings: The embeddings model to use for FAISS indexing\n",
    "        persist_path: Path to save the FAISS index\n",
    "\n",
    "    Returns:\n",
    "        FAISS vector store\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Check if FAISS index already exists\n",
    "    if os.path.exists(os.path.join(persist_path, \"index.faiss\")):\n",
    "        print(f\"üîÑ Loading existing FAISS index from '{persist_path}'\")\n",
    "        # Load existing index\n",
    "        vectorstore = FAISS.load_local(persist_path, embeddings, allow_dangerous_deserialization=True)\n",
    "        \n",
    "        # Get existing document IDs to avoid duplicates\n",
    "        existing_ids = set(vectorstore.docstore._dict.keys())\n",
    "        \n",
    "        # Filter out documents that might already exist in the index\n",
    "        # This assumes documents have unique IDs in their metadata\n",
    "        new_docs = []\n",
    "        for doc in documents:\n",
    "            # Use chunk_id as a unique identifier if available\n",
    "            doc_id = doc.metadata.get(\"chunk_id\", None)\n",
    "            if doc_id and doc_id not in existing_ids:\n",
    "                new_docs.append(doc)\n",
    "        \n",
    "        if new_docs:\n",
    "            print(f\"‚ûï Adding {len(new_docs)} new documents to existing index\")\n",
    "            vectorstore.add_documents(new_docs)\n",
    "            # Save the updated index\n",
    "            vectorstore.save_local(persist_path)\n",
    "            print(f\"‚úÖ FAISS index updated and saved to '{persist_path}'\\n\")\n",
    "        else:\n",
    "            print(f\"‚ÑπÔ∏è No new documents to add to the index\\n\")\n",
    "    else:\n",
    "        print(f\"üÜï Creating new FAISS index\")\n",
    "        # Build the index in memory with all documents\n",
    "        vectorstore = FAISS.from_documents(documents, embedding=embeddings)\n",
    "        # Save to disk\n",
    "        vectorstore.save_local(persist_path)\n",
    "        print(f\"‚úÖ FAISS index built and saved to '{persist_path}'\\n\")\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_FOLDER_PATH = \"./output/chunked_text/\"\n",
    "OUTPUT_FOLDER_PATH = \"./output/faiss_index/\"\n",
    "os.makedirs(os.path.abspath(OUTPUT_FOLDER_PATH), exist_ok=True)\n",
    "LIMIT = 0\n",
    "n_embedded = 0\n",
    "\n",
    "\n",
    "already_processed = set()\n",
    "with open(os.path.join(OUTPUT_FOLDER_PATH, \"processed_files.txt\"), \"a+\") as f:\n",
    "    f.seek(0)\n",
    "    for line in f:\n",
    "        already_processed.add(line.strip())\n",
    "\n",
    "for file_name in os.listdir(SRC_FOLDER_PATH):\n",
    "    if file_name.endswith('.json') and file_name not in already_processed and (n_embedded < LIMIT or LIMIT == -1):\n",
    "        file_path = os.path.join(SRC_FOLDER_PATH, file_name)\n",
    "        chunks = load_json_as_dict_list(file_path)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(chunks)} chunks from {file_path}\")\n",
    "        \n",
    "        if len(chunks) > 100:\n",
    "            for i in range(0, len(chunks), 100):\n",
    "                documents = prepare_documents(chunks[i:i + 100])\n",
    "                build_faiss_index(documents, embeddings)\n",
    "        else:\n",
    "            documents = prepare_documents(chunks)\n",
    "            build_faiss_index(documents, embeddings)\n",
    "\n",
    "        with open(os.path.join(OUTPUT_FOLDER_PATH, \"processed_files.txt\"), \"a\") as f:\n",
    "            f.write(file_name + \"\\n\")\n",
    "        \n",
    "        logger.info(f\"Processed and indexed {file_name}\")\n",
    "        n_embedded += 1\n",
    "    \n",
    "    if n_embedded >= LIMIT and LIMIT != -1:\n",
    "        logger.info(f\"Reached limit of {LIMIT} files processed. Stopping.\")\n",
    "        break\n",
    "\n",
    "#documents = prepare_documents(chunks)\n",
    "#build_faiss_index(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f5459",
   "metadata": {},
   "source": [
    "#### 3.3. Visualize the FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf2833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from visualizer import visualize_2D_colored\n",
    "from io_docs import load_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d28ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore, documents, all_embeddings, metadatas = load_vectorstore(FAISS_PATH, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a32ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_2D_colored(all_embeddings, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete docs or ids...\n",
    "def delete_from_faiss(doc_name:str):\n",
    "    vectorstore = FAISS.load_local(FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"before: \", vectorstore.index.ntotal)\n",
    "    documents = list(vectorstore.docstore._dict.values())\n",
    "    for doc in documents:\n",
    "        if doc.metadata[\"doc_name\"] == doc_name:\n",
    "            print(doc.metadata[\"chunk_id\"])\n",
    "            vectorstore.delete(ids=[doc.id])\n",
    "\n",
    "    vectorstore.save_local(FAISS_PATH)\n",
    "    print(\"after: \", vectorstore.index.ntotal)\n",
    "\n",
    "    visualize_2D_colored()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6963be",
   "metadata": {},
   "source": [
    "#### 4. Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d1bb7b",
   "metadata": {},
   "source": [
    "##### 4.1. Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc287ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io_docs import load_faiss_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9895b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks(query: str, k: int = 10, vectorstore: FAISS = load_faiss_index(FAISS_PATH, embeddings)) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Embeds the query and retrieves top-k semantically similar chunks from FAISS.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "        k (int): The number of similar chunks to retrieve.\n",
    "        embeddings (HuggingFaceEmbeddings): The embeddings model to use.\n",
    "\n",
    "    Returns: \n",
    "        List of dicts with chunk text + metadata.\n",
    "    \"\"\"\n",
    "    results = vectorstore.similarity_search(query, k=k)\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"text\": doc.page_content,\n",
    "            \"doc_name\": doc.metadata.get(\"doc_name\", ''),\n",
    "            \"page_num\": doc.metadata.get(\"page_num\", -1),\n",
    "            \"chunk_id\": doc.metadata.get(\"chunk_id\", ''),\n",
    "            \"n_words\": doc.metadata.get(\"num_words\", -1)\n",
    "        }\n",
    "        for doc in results\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c566e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = load_faiss_index(FAISS_PATH, embeddings)\n",
    "query = \"D√©finir une aiguille\"\n",
    "k = 5\n",
    "res = search_similar_chunks(query, k, vectorstore)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76fc82",
   "metadata": {},
   "source": [
    "#### 5. Answer generation using LM Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab49961",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT1 = f\"\"\"\n",
    "En tant qu'ancien collaborateur de la Compagnie des Signaux, sp√©cialis√© dans la signalisation ferroviaire, vous poss√©dez une expertise compl√®te des termes techniques et des documents internes du secteur.\n",
    "En utilisant uniquement les informations contenues dans les documents internes ci-apr√®s, r√©pondez concisement √† la question suivante avec une r√©ponse claire pr√©cise et technique.\n",
    "Si les informations disponibles ne suffisent pas, indiquez-le clairement. N'inventez aucune r√©ponse.\n",
    "\n",
    "### Informations R√©f√©rences :\n",
    "{{context}}\n",
    "\n",
    "### Questions :\n",
    "{{query}}\n",
    "\n",
    "### R√©ponse :\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd00083",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT2 = \"\"\"\n",
    "Vous √™tes un ancien collaborateur de la Compagnie des Signaux, expert en signalisation ferroviaire. \n",
    "Votre r√¥le est d‚Äôanalyser les documents internes fournis et de r√©pondre de mani√®re claire, pr√©cise et techniquement correcte.\n",
    "\n",
    "R√®gles imp√©ratives :\n",
    "1. Utilisez uniquement les informations figurant dans la section \"Informations R√©f√©rences\".\n",
    "2. Ne d√©duisez ou n‚Äôinventez rien qui ne soit pas explicitement mentionn√©.\n",
    "3. Si les informations sont insuffisantes pour r√©pondre, √©crivez uniquement : \"Informations insuffisantes pour r√©pondre.\"\n",
    "4. La r√©ponse doit √™tre concise, technique, et directement li√©e √† la question.\n",
    "5. Ne fournissez aucune opinion personnelle ou information externe.\n",
    "6. Respectez le vocabulaire technique du domaine ferroviaire.\n",
    "\n",
    "### Informations R√©f√©rences :\n",
    "{{context}}\n",
    "\n",
    "### Question :\n",
    "{{query}}\n",
    "\n",
    "### R√©ponse :\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9df4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_with_lmstudio(query: str, context_chunks: list, prompt:str = PROMPT1, max_tokens: int = 200) -> str:\n",
    "    \"\"\"\n",
    "    Sends a prompt to LM Studio's local API and returns the generated answer.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt template with placeholders for context and query.\n",
    "        query (str): The search query string.\n",
    "        context_chunks (list): List of context chunks to include in the prompt.\n",
    "        max_tokens (int): Maximum number of tokens to generate in the response.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer from LM Studio.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build RAG-style prompt\n",
    "    context_text = \"\\n\\n\".join(list(map(lambda c: c.get(\"text\", \"\"), context_chunks)))\n",
    "    prompt = prompt.replace(\"{context}\", context_text)\n",
    "    prompt = prompt.replace(\"{query}\", query)\n",
    "\n",
    "    # Call LM Studio's local API\n",
    "    response = requests.post(\"http://localhost:1234/v1/completions\", json={\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.2,\n",
    "        \"stop\": None,\n",
    "    })\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error: {response.status_code} {response.text}\")\n",
    "\n",
    "    return response.json()[\"choices\"][0][\"text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37eea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = search_similar_chunks(query, k, vectorstore)\n",
    "response = generate_answer_with_lmstudio(query, ctx, prompt=PROMPT2, max_tokens=200)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
