{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ed59e9c",
   "metadata": {},
   "source": [
    "# Embedding Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a4e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(\"../prod/utils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae2ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & utils\n",
    "from io_docs import load_embeddings\n",
    "from embeddings import get_embedding_model\n",
    "from logging_config import logger\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances, silhouette_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import skew\n",
    "np.random.seed(42)\n",
    "\n",
    "def l2_normalize(X, eps=1e-12):\n",
    "    n = np.linalg.norm(X, axis=1, keepdims=True) + eps\n",
    "    return X / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bfe1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "FAISS_PATH = \"../prod/faiss_index\"\n",
    "embeddings = get_embedding_model(1) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19897bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "documents, all_embeddings, metadatas = load_embeddings(FAISS_PATH, embeddings)\n",
    "\n",
    "X = np.array(all_embeddings)\n",
    "n, d = X.shape\n",
    "\n",
    "# Optional metadata (delete if not available)\n",
    "section_labels = None # np.array(metadatas).astype(object)               # e.g., np.load(\"section_labels.npy\").astype(object)\n",
    "doc_order = np.arange(n)            # 0..n-1 in reading order; shuffle to match X if needed\n",
    "\n",
    "X = l2_normalize(X)\n",
    "X.shape, (section_labels.shape if section_labels is not None else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5fd282",
   "metadata": {},
   "source": [
    "## 1. Pairwise Cosine Similarity Distribution\n",
    "**Formula:**  \n",
    "cosine_sim(x, y) = (x · y) / (||x|| * ||y||)\n",
    "\n",
    "**Why:** Checks if embeddings are meaningfully spread out.  \n",
    "**Rule of thumb:**  \n",
    "- Mean ~0.0–0.2 → good  \n",
    "- Mean >0.4 → likely collapse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f89a1b",
   "metadata": {},
   "source": [
    "## 2. Cosine to Mean Vector (Anisotropy)\n",
    "**Formula:**  \n",
    "μ = mean(x₁, x₂, …, xₙ)  \n",
    "cos_to_mean(i) = cosine_sim(xᵢ, μ)\n",
    "\n",
    "**Why:** Detects if most embeddings point in same direction.  \n",
    "**Rule of thumb:**  \n",
    "- Mean ≤0.10–0.15 → fine  \n",
    "- ≥0.25 → strong anisotropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4cdc38",
   "metadata": {},
   "source": [
    "## 3. First Principal Component Variance Ratio\n",
    "**Formula:**  \n",
    "variance_ratio₁ = λ₁ / (λ₁ + λ₂ + … + λ_d)  \n",
    "(λ₁ = variance along first PC)\n",
    "\n",
    "**Why:** High values = one dominant direction in space.  \n",
    "**Rule of thumb:**  \n",
    "- <0.25 → good  \n",
    "- 0.25–0.5 → moderate anisotropy  \n",
    "- \\>0.5 → strong anisotropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1aca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Pairwise cosine sample to avoid O(n^2)\n",
    "m = min(2000, n)  # sample size\n",
    "idx = np.random.choice(n, size=m, replace=False)\n",
    "Xs = X[idx]\n",
    "\n",
    "cos_dists = pairwise_distances(Xs, metric=\"cosine\")\n",
    "cos_sims = 1 - cos_dists\n",
    "mask = np.triu(np.ones_like(cos_sims, dtype=bool), k=1)\n",
    "pair_cos = cos_sims[mask]\n",
    "\n",
    "# Isotropy (cosine to mean direction)\n",
    "mu = X.mean(axis=0, keepdims=True)\n",
    "mu = l2_normalize(mu)\n",
    "cos_to_mean = (X @ mu.T).ravel()\n",
    "\n",
    "# PCA anisotropy\n",
    "pca = PCA(n_components=10, svd_solver=\"randomized\", random_state=42).fit(X)\n",
    "first_pc_ratio = pca.explained_variance_ratio_[0]\n",
    "\n",
    "print(f\"Pairwise cosine: mean={pair_cos.mean():.3f}, std={pair_cos.std():.3f}, q95={np.quantile(pair_cos,0.95):.3f}\")\n",
    "print(f\"Cosine to mean vector: mean={cos_to_mean.mean():.3f}, std={cos_to_mean.std():.3f}\")\n",
    "print(f\"First PC variance ratio: {first_pc_ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb81d24",
   "metadata": {},
   "source": [
    "## 4. Hubness\n",
    "**Definition:** Count how often each vector appears in others’ top-k neighbors.  \n",
    "Check **skewness** and **Gini coefficient**.\n",
    "\n",
    "**Why:** High hubness means a few vectors appear in many neighbor lists, distorting search.  \n",
    "**Rule of thumb:**  \n",
    "- Skew ≤1.0, Gini ≤0.3 → healthy  \n",
    "- Skew >2.0 or Gini >0.4 → hubness problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d088429",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "nn = NearestNeighbors(n_neighbors=k+1, metric=\"cosine\").fit(X)\n",
    "_, inds = nn.kneighbors(X, n_neighbors=k+1)\n",
    "inds = inds[:,1:]  # drop self\n",
    "\n",
    "# Count how often each point is a neighbor of others\n",
    "counts = np.zeros(n, dtype=int)\n",
    "for row in inds:\n",
    "    for j in row:\n",
    "        counts[j] += 1\n",
    "\n",
    "def gini(x):\n",
    "    x = np.sort(x.astype(np.float64))\n",
    "    if x.sum() == 0: return 0.0\n",
    "    n = len(x)\n",
    "    cumx = np.cumsum(x)\n",
    "    return (n + 1 - 2 * (cumx / x.sum()).sum() / n)\n",
    "\n",
    "print(f\"k={k} mean-occurrence={counts.mean():.2f}, max={counts.max()}, skew={skew(counts):.2f}, gini={gini(counts):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0969ea",
   "metadata": {},
   "source": [
    "## 5. Adjacency@k (Sequential Coherence)\n",
    "**Definition:** For document chunks, % whose previous/next chunk appears in their top-k neighbors.\n",
    "\n",
    "**Why:** Adjacent chunks in text should be semantically close.  \n",
    "**Rule of thumb:**  \n",
    "- ≥0.6 → strong coherence  \n",
    "- 0.3–0.6 → moderate  \n",
    "- <0.3 → poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179c8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacent coherence\n",
    "def adjacency_at_k(inds, order):\n",
    "    pos = np.empty_like(order)\n",
    "    pos[order] = np.arange(len(order))\n",
    "    hits = 0\n",
    "    total = 0\n",
    "    for i in range(len(order)):\n",
    "        neighbors = set(inds[i])\n",
    "        # previous / next in reading order\n",
    "        p = i-1 if i-1 >= 0 else None\n",
    "        q = i+1 if i+1 < len(order) else None\n",
    "        hit = False\n",
    "        if p is not None: hit |= (p in neighbors)\n",
    "        if q is not None: hit |= (q in neighbors)\n",
    "        hits += int(hit)\n",
    "        total += 1\n",
    "    return hits / total\n",
    "\n",
    "adj10 = adjacency_at_k(inds, doc_order)\n",
    "print(f\"Adjacency@{k}: {adj10:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a52a82",
   "metadata": {},
   "source": [
    "## 6. Section Purity@k (Label-Based Coherence)\n",
    "**Definition:** For each chunk, fraction of k-NN that share the same section/topic label.\n",
    "\n",
    "**Why:** Checks topical grouping in embeddings.  \n",
    "**Rule of thumb:**  \n",
    "- ≥0.7 → strong separation  \n",
    "- 0.5–0.7 → moderate  \n",
    "- <0.5 → weak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section purity (skip if no labels)\n",
    "if section_labels is not None:\n",
    "    labels = np.asarray(section_labels)\n",
    "    purities = []\n",
    "    for i in range(n):\n",
    "        nbrs = inds[i]\n",
    "        purities.append(np.mean(labels[nbrs] == labels[i]))\n",
    "    print(f\"Section Purity@{k}: {np.mean(purities):.3f}\")\n",
    "else:\n",
    "    print(\"No section_labels provided — skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c7250",
   "metadata": {},
   "source": [
    "## 7. Silhouette Score (if labels available)\n",
    "**Formula:**  \n",
    "a(i) = avg distance to same-cluster points  \n",
    "b(i) = smallest avg distance to a different cluster  \n",
    "silhouette(i) = (b(i) − a(i)) / max(a(i), b(i))\n",
    "\n",
    "**Why:** Measures separation quality given labels.  \n",
    "**Rule of thumb:**  \n",
    "- ≥0.5 → strong  \n",
    "- 0.3–0.5 → moderate  \n",
    "- <0.3 → weak (0.2–0.3 still common for text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9418de0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b1825f0",
   "metadata": {},
   "source": [
    "## 8. Retrieval Metrics (RAG-Focused)\n",
    "**Recall@k:** relevant_found_in_top_k / total_relevant  \n",
    "**Precision@k:** relevant_found_in_top_k / k  \n",
    "**MRR:** mean(1 / rank_of_first_relevant)  \n",
    "**nDCG@k:** discounted gain of ranked results vs ideal order\n",
    "\n",
    "**Why:** Directly measures retrieval quality for your use case.  \n",
    "**Rule of thumb:**  \n",
    "- Recall@10 ≥0.7 → good  \n",
    "- MRR ≥0.5 → relevant appears very early  \n",
    "- nDCG@10 ≥0.6 → good ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f3a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval evaluation\n",
    "# Inputs:\n",
    "# Q: (num_queries, d) normalized embeddings of queries\n",
    "# gt: list[set[int]] relevant chunk indices per query\n",
    "\n",
    "def topk_cosine_search(Q, X, k=10):\n",
    "    # X, Q assumed L2-normalized; cosine sim = dot product\n",
    "    sims = Q @ X.T   # (q, n)\n",
    "    idx = np.argpartition(-sims, kth=k-1, axis=1)[:,:k]\n",
    "    # sort top-k per row\n",
    "    row_scores = np.take_along_axis(sims, idx, axis=1)\n",
    "    order = np.argsort(-row_scores, axis=1)\n",
    "    return np.take_along_axis(idx, order, axis=1), np.take_along_axis(row_scores, order, axis=1)\n",
    "\n",
    "def retrieval_metrics_at_k(topk, gt_sets, k=10):\n",
    "    q = len(gt_sets)\n",
    "    recall = precision = mrr = ndcg = 0.0\n",
    "    for i in range(q):\n",
    "        hits = [1 if r in gt_sets[i] else 0 for r in topk[i,:k]]\n",
    "        # P@k, R@k\n",
    "        precision += sum(hits)/k\n",
    "        recall += (sum(hits)/max(1, len(gt_sets[i])))\n",
    "        # MRR\n",
    "        rank = next((j+1 for j,h in enumerate(hits) if h==1), None)\n",
    "        if rank: mrr += 1.0/rank\n",
    "        # nDCG@k\n",
    "        dcg = sum(h/np.log2(j+2) for j,h in enumerate(hits))\n",
    "        ideal = sum(1/np.log2(j+2) for j in range(min(k, len(gt_sets[i]))))\n",
    "        ndcg += (dcg/ideal if ideal>0 else 0.0)\n",
    "    return dict(\n",
    "    recall_at_k=recall/q,\n",
    "    precision_at_k=precision/q,\n",
    "    mrr=mrr/q,\n",
    "    ndcg_at_k=ndcg/q\n",
    ")\n",
    "\n",
    "\n",
    "# --- Demo with synthetic data (replace with real Q and gt) ---\n",
    "# Q = np.load(\"query_embeddings.npy\"); Q = l2_normalize(Q)\n",
    "# gt = [set([12, 98]), set([3, 17, 21]), ...]\n",
    "Q = l2_normalize(np.random.normal(size=(10, d)).astype(np.float32))\n",
    "gt = [set(np.random.choice(n, size=np.random.randint(1,4), replace=False)) for _ in range(len(Q))]\n",
    "topk, scores = topk_cosine_search(Q, X, k=10)\n",
    "metrics = retrieval_metrics_at_k(topk, gt, k=10)\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
