{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3889039a",
   "metadata": {},
   "source": [
    "# RAG SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4cb4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the script environment\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append('./utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd48dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "from typing import List, Dict, Optional, Any\n",
    "from logging_config import logger\n",
    "from io_docs import save_dict_list_as_json, load_json_as_dict_list, dict_list_to_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b812288",
   "metadata": {},
   "source": [
    "## 1. Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574311ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from utils.text_extractor import TextExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e4c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_FOLDER_PATH = \"../../src/first_batch/\"\n",
    "OUTPUT_FOLDER_PATH = \"./output/extracted_text/\"\n",
    "LIMIT = 0\n",
    "n_extracted = 0\n",
    "\n",
    "for file_name in os.listdir(SRC_FOLDER_PATH):\n",
    "    if file_name.endswith('.pdf') and (n_extracted < LIMIT or LIMIT == -1):\n",
    "        file_path = os.path.join(SRC_FOLDER_PATH, file_name)\n",
    "\n",
    "        if f\"{os.path.basename(file_path).split('.')[0]}_extracted.json\" in os.listdir(OUTPUT_FOLDER_PATH):\n",
    "            logger.info(f\"Skipping already processed file: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"Processing file: {file_path}\")\n",
    "\n",
    "        # Initialize the TextExtractor with the file path and parameters\n",
    "        text_extractor = TextExtractor(file_path, min_words=14)\n",
    "        \n",
    "        # Extract text from the PDF\n",
    "        text_extractor.extract_text_advanced()\n",
    "        \n",
    "        # Clean the extracted text\n",
    "        text_extractor.clean_text()\n",
    "\n",
    "        # Saving extracted text to folder\n",
    "        os.makedirs(OUTPUT_FOLDER_PATH, exist_ok=True)\n",
    "        output_file_path = os.path.join(OUTPUT_FOLDER_PATH, f\"{os.path.basename(file_path).split('.')[0]}_extracted.json\")\n",
    "        save_dict_list_as_json(text_extractor.get_data(), output_file_path)\n",
    "\n",
    "        n_extracted += 1\n",
    "\n",
    "    if n_extracted >= LIMIT and LIMIT != -1:\n",
    "        logger.info(f\"Reached limit of {LIMIT} files processed. Stopping.\")\n",
    "        break\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad82ec7",
   "metadata": {},
   "source": [
    "## 2. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebdf73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from embeddings import get_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c21378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_semantic(pages: List[Dict], embeddings, breakpoint_threshold_type = \"percentile\", breakpoint_threshold_amount: float = 95, min_chunk_size: int = 100) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Splits page texts using LangChain's SemanticChunker.\n",
    "    Groups sentences based on semantic similarity, not just size.\n",
    "    Information on how to use the chunker can be found in the LangChain documentation: https://python.langchain.com/docs/how_to/semantic-chunker/\n",
    "\n",
    "    Parameters:\n",
    "        pages: List of dicts with keys: doc_name, page_num, text\n",
    "        embeddings: The embeddings model to use for semantic chunking\n",
    "        breakpoint_threshold_type: Type of threshold for chunking (default is \"percentile\")\n",
    "        breakpoint_threshold_amount: Amount for the threshold (default is 95)\n",
    "        min_chunk_size: Minimum size of chunks (default is 100)\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with chunked text and metadata\n",
    "    \"\"\"\n",
    "    chunker = SemanticChunker(embeddings, breakpoint_threshold_type=breakpoint_threshold_type, breakpoint_threshold_amount=breakpoint_threshold_amount, min_chunk_size=min_chunk_size) # type: ignore\n",
    "\n",
    "    chunks = []\n",
    "    doc_name = pages[0].get(\"doc_name\", \"unknown\") if pages else \"unknown\"\n",
    "\n",
    "    for page in pages:\n",
    "        page_num = page.get(\"page_number\", -1)\n",
    "        try:\n",
    "            text = page.get(\"text\", \"\")\n",
    "            chunked_texts = chunker.split_text(text)\n",
    "            for i, chunk in enumerate(chunked_texts):\n",
    "                chunks.append({\n",
    "                    \"doc_name\": doc_name,\n",
    "                    \"page_number\": page_num,\n",
    "                    \"chunk_id\": f\"{doc_name}_p{page_num}_c{i}\",\n",
    "                    \"text\": chunk,\n",
    "                    \"num_words\": len(chunk.split()),\n",
    "                    \"extraction_method\": page.get(\"extraction_method\", \"unknown\"),\n",
    "                    \"table_bboxes\": page.get(\"table_bboxes\", [])\n",
    "                })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"⚠️ Semantic chunking failed on {doc_name} page {page_num}: {e}\")\n",
    "\n",
    "    logger.info(f\"✅ Created {len(chunks)} semantic chunks from {len(pages)} pages in {doc_name}.\\n\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbb488",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embedding_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a7464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_FOLDER_PATH = \"./output/extracted_text/\"\n",
    "OUTPUT_FOLDER_PATH = \"./output/chunked_text/\"\n",
    "LIMIT = 0\n",
    "n_chunked = 0\n",
    "\n",
    "os.makedirs(OUTPUT_FOLDER_PATH, exist_ok=True)\n",
    "\n",
    "for i, doc in enumerate(os.listdir(SRC_FOLDER_PATH)):\n",
    "    if doc.endswith('.json') and (n_chunked < LIMIT or LIMIT == -1):\n",
    "        file_path = os.path.join(SRC_FOLDER_PATH, doc)\n",
    "\n",
    "        if f\"{os.path.basename(file_path).split('.')[0]}_chunked.json\" in os.listdir(OUTPUT_FOLDER_PATH):\n",
    "            logger.info(f\"Skipping already processed file: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"Processing file: {file_path}\")\n",
    "\n",
    "        # Load the extracted text data\n",
    "        doc_pages = load_json_as_dict_list(file_path)\n",
    "\n",
    "        # Chunk the text semantically\n",
    "        chunks = chunk_text_semantic(doc_pages, embeddings, min_chunk_size=200, breakpoint_threshold_type=\"gradient\")\n",
    "\n",
    "        # Save the chunks to a new JSON file\n",
    "        output_file_path = os.path.join(OUTPUT_FOLDER_PATH, f\"{os.path.basename(file_path).split('.')[0]}_chunked.json\")\n",
    "        save_dict_list_as_json(chunks, output_file_path)\n",
    "\n",
    "        n_chunked += 1\n",
    "\n",
    "    if n_chunked >= LIMIT and LIMIT != -1:\n",
    "        logger.info(f\"Reached limit of {LIMIT} files processed. Stopping.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f34aa4",
   "metadata": {},
   "source": [
    "## 3. Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd991dd",
   "metadata": {},
   "source": [
    "#### 3.1. Documents preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccd3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f94b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting chunks into LangChain Documents\n",
    "def prepare_documents(chunks: List[Dict]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Converts each chunk dict to a LangChain Document object\n",
    "    with metadata (doc_name, page_num, chunk_id)\n",
    "\n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries\n",
    "\n",
    "    Returns:\n",
    "        List of LangChain Document objects\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for chunk in chunks:\n",
    "        metadata = {\n",
    "            \"doc_name\": chunk.get(\"doc_name\", \"unknown\"),\n",
    "            \"page_number\": chunk.get(\"page_number\", -1),\n",
    "            \"chunk_id\": chunk.get(\"chunk_id\", \"unknown\"),\n",
    "            \"text\": chunk.get(\"text\", \"\"),\n",
    "            \"num_words\": chunk.get(\"num_words\", 0),\n",
    "            \"extraction_method\": chunk.get(\"extraction_method\", \"unknown\"),\n",
    "            \"table_bboxes\": chunk.get(\"table_bboxes\", []),\n",
    "        }\n",
    "        documents.append(Document(page_content=chunk[\"text\"], metadata=metadata))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb037a7",
   "metadata": {},
   "source": [
    "#### 3.2. Build the FAISS index from Documents using the embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "FAISS_PATH = \"faiss_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e69e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the FAISS index from Documents\n",
    "def build_faiss_index(documents: List[Document], embeddings: HuggingFaceEmbeddings, persist_path: str = FAISS_PATH) -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates or updates a FAISS vector store from a list of LangChain Documents.\n",
    "    If the index already exists, only adds new documents instead of recreating it.\n",
    "    Saves it to disk for future use.\n",
    "\n",
    "    Args:\n",
    "        documents: List of LangChain Document objects\n",
    "        embeddings: The embeddings model to use for FAISS indexing\n",
    "        persist_path: Path to save the FAISS index\n",
    "\n",
    "    Returns:\n",
    "        FAISS vector store\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Check if FAISS index already exists\n",
    "    if os.path.exists(os.path.join(persist_path, \"index.faiss\")):\n",
    "        print(f\"🔄 Loading existing FAISS index from '{persist_path}'\")\n",
    "        # Load existing index\n",
    "        vectorstore = FAISS.load_local(persist_path, embeddings, allow_dangerous_deserialization=True)\n",
    "        \n",
    "        # Get existing document IDs to avoid duplicates\n",
    "        existing_ids = set(vectorstore.docstore._dict.keys())\n",
    "        \n",
    "        # Filter out documents that might already exist in the index\n",
    "        # This assumes documents have unique IDs in their metadata\n",
    "        new_docs = []\n",
    "        for doc in documents:\n",
    "            # Use chunk_id as a unique identifier if available\n",
    "            doc_id = doc.metadata.get(\"chunk_id\", None)\n",
    "            if doc_id and doc_id not in existing_ids:\n",
    "                new_docs.append(doc)\n",
    "        \n",
    "        if new_docs:\n",
    "            print(f\"➕ Adding {len(new_docs)} new documents to existing index\")\n",
    "            vectorstore.add_documents(new_docs)\n",
    "            # Save the updated index\n",
    "            vectorstore.save_local(persist_path)\n",
    "            print(f\"✅ FAISS index updated and saved to '{persist_path}'\\n\")\n",
    "        else:\n",
    "            print(f\"ℹ️ No new documents to add to the index\\n\")\n",
    "    else:\n",
    "        print(f\"🆕 Creating new FAISS index\")\n",
    "        # Build the index in memory with all documents\n",
    "        vectorstore = FAISS.from_documents(documents, embedding=embeddings)\n",
    "        # Save to disk\n",
    "        vectorstore.save_local(persist_path)\n",
    "        print(f\"✅ FAISS index built and saved to '{persist_path}'\\n\")\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_FOLDER_PATH = \"./output/chunked_text/\"\n",
    "OUTPUT_FOLDER_PATH = \"./output/faiss_index/\"\n",
    "os.makedirs(os.path.abspath(OUTPUT_FOLDER_PATH), exist_ok=True)\n",
    "LIMIT = 0\n",
    "n_embedded = 0\n",
    "\n",
    "\n",
    "already_processed = set()\n",
    "with open(os.path.join(OUTPUT_FOLDER_PATH, \"processed_files.txt\"), \"a+\") as f:\n",
    "    f.seek(0)\n",
    "    for line in f:\n",
    "        already_processed.add(line.strip())\n",
    "\n",
    "for file_name in os.listdir(SRC_FOLDER_PATH):\n",
    "    if file_name.endswith('.json') and file_name not in already_processed and (n_embedded < LIMIT or LIMIT == -1):\n",
    "        file_path = os.path.join(SRC_FOLDER_PATH, file_name)\n",
    "        chunks = load_json_as_dict_list(file_path)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(chunks)} chunks from {file_path}\")\n",
    "        \n",
    "        if len(chunks) > 100:\n",
    "            for i in range(0, len(chunks), 100):\n",
    "                documents = prepare_documents(chunks[i:i + 100])\n",
    "                build_faiss_index(documents, embeddings)\n",
    "        else:\n",
    "            documents = prepare_documents(chunks)\n",
    "            build_faiss_index(documents, embeddings)\n",
    "\n",
    "        with open(os.path.join(OUTPUT_FOLDER_PATH, \"processed_files.txt\"), \"a\") as f:\n",
    "            f.write(file_name + \"\\n\")\n",
    "        \n",
    "        logger.info(f\"Processed and indexed {file_name}\")\n",
    "        n_embedded += 1\n",
    "    \n",
    "    if n_embedded >= LIMIT and LIMIT != -1:\n",
    "        logger.info(f\"Reached limit of {LIMIT} files processed. Stopping.\")\n",
    "        break\n",
    "\n",
    "#documents = prepare_documents(chunks)\n",
    "#build_faiss_index(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f5459",
   "metadata": {},
   "source": [
    "#### 3.3. Visualize the FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf2833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from visualizer import visualize_2D_colored\n",
    "from io_docs import load_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d28ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore, documents, all_embeddings, metadatas = load_vectorstore(FAISS_PATH, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a32ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_2D_colored(all_embeddings, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete docs or ids...\n",
    "def delete_from_faiss(doc_name:str):\n",
    "    vectorstore = FAISS.load_local(FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"before: \", vectorstore.index.ntotal)\n",
    "    documents = list(vectorstore.docstore._dict.values())\n",
    "    for doc in documents:\n",
    "        if doc.metadata[\"doc_name\"] == doc_name:\n",
    "            print(doc.metadata[\"chunk_id\"])\n",
    "            vectorstore.delete(ids=[doc.id])\n",
    "\n",
    "    vectorstore.save_local(FAISS_PATH)\n",
    "    print(\"after: \", vectorstore.index.ntotal)\n",
    "\n",
    "    visualize_2D_colored()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6963be",
   "metadata": {},
   "source": [
    "#### 4. Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d1bb7b",
   "metadata": {},
   "source": [
    "##### 4.1. Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc287ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io_docs import load_faiss_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9895b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks(query: str, k: int = 10, vectorstore: FAISS = load_faiss_index(FAISS_PATH, embeddings)) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Embeds the query and retrieves top-k semantically similar chunks from FAISS.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "        k (int): The number of similar chunks to retrieve.\n",
    "        embeddings (HuggingFaceEmbeddings): The embeddings model to use.\n",
    "\n",
    "    Returns: \n",
    "        List of dicts with chunk text + metadata.\n",
    "    \"\"\"\n",
    "    results = vectorstore.similarity_search(query, k=k)\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"text\": doc.page_content,\n",
    "            \"doc_name\": doc.metadata.get(\"doc_name\", ''),\n",
    "            \"page_num\": doc.metadata.get(\"page_num\", -1),\n",
    "            \"chunk_id\": doc.metadata.get(\"chunk_id\", ''),\n",
    "            \"n_words\": doc.metadata.get(\"num_words\", -1)\n",
    "        }\n",
    "        for doc in results\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c566e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = load_faiss_index(FAISS_PATH, embeddings)\n",
    "query = \"Définir une aiguille\"\n",
    "k = 5\n",
    "res = search_similar_chunks(query, k, vectorstore)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76fc82",
   "metadata": {},
   "source": [
    "#### 5. Answer generation using LM Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab49961",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT1 = f\"\"\"\n",
    "En tant qu'ancien collaborateur de la Compagnie des Signaux, spécialisé dans la signalisation ferroviaire, vous possédez une expertise complète des termes techniques et des documents internes du secteur.\n",
    "En utilisant uniquement les informations contenues dans les documents internes ci-après, répondez concisement à la question suivante avec une réponse claire précise et technique.\n",
    "Si les informations disponibles ne suffisent pas, indiquez-le clairement. N'inventez aucune réponse.\n",
    "\n",
    "### Informations Références :\n",
    "{{context}}\n",
    "\n",
    "### Questions :\n",
    "{{query}}\n",
    "\n",
    "### Réponse :\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd00083",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT2 = \"\"\"\n",
    "Vous êtes un ancien collaborateur de la Compagnie des Signaux, expert en signalisation ferroviaire. \n",
    "Votre rôle est d’analyser les documents internes fournis et de répondre de manière claire, précise et techniquement correcte.\n",
    "\n",
    "Règles impératives :\n",
    "1. Utilisez uniquement les informations figurant dans la section \"Informations Références\".\n",
    "2. Ne déduisez ou n’inventez rien qui ne soit pas explicitement mentionné.\n",
    "3. Si les informations sont insuffisantes pour répondre, écrivez uniquement : \"Informations insuffisantes pour répondre.\"\n",
    "4. La réponse doit être concise, technique, et directement liée à la question.\n",
    "5. Ne fournissez aucune opinion personnelle ou information externe.\n",
    "6. Respectez le vocabulaire technique du domaine ferroviaire.\n",
    "\n",
    "### Informations Références :\n",
    "{{context}}\n",
    "\n",
    "### Question :\n",
    "{{query}}\n",
    "\n",
    "### Réponse :\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9df4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_with_lmstudio(query: str, context_chunks: list, prompt:str = PROMPT1, max_tokens: int = 200) -> str:\n",
    "    \"\"\"\n",
    "    Sends a prompt to LM Studio's local API and returns the generated answer.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt template with placeholders for context and query.\n",
    "        query (str): The search query string.\n",
    "        context_chunks (list): List of context chunks to include in the prompt.\n",
    "        max_tokens (int): Maximum number of tokens to generate in the response.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer from LM Studio.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build RAG-style prompt\n",
    "    context_text = \"\\n\\n\".join(list(map(lambda c: c.get(\"text\", \"\"), context_chunks)))\n",
    "    prompt = prompt.replace(\"{context}\", context_text)\n",
    "    prompt = prompt.replace(\"{query}\", query)\n",
    "\n",
    "    # Call LM Studio's local API\n",
    "    response = requests.post(\"http://localhost:1234/v1/completions\", json={\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.2,\n",
    "        \"stop\": None,\n",
    "    })\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error: {response.status_code} {response.text}\")\n",
    "\n",
    "    return response.json()[\"choices\"][0][\"text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37eea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = search_similar_chunks(query, k, vectorstore)\n",
    "response = generate_answer_with_lmstudio(query, ctx, prompt=PROMPT2, max_tokens=200)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
